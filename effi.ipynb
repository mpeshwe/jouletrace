{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3581479",
   "metadata": {},
   "source": [
    "## Effibench Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f3cbc5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /home/cc/miniconda3/envs/greenllm/lib/python3.10/site-packages (4.1.1)\n",
      "Requirement already satisfied: filelock in /home/cc/miniconda3/envs/greenllm/lib/python3.10/site-packages (from datasets) (3.19.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/cc/miniconda3/envs/greenllm/lib/python3.10/site-packages (from datasets) (2.2.6)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /home/cc/miniconda3/envs/greenllm/lib/python3.10/site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /home/cc/miniconda3/envs/greenllm/lib/python3.10/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /home/cc/miniconda3/envs/greenllm/lib/python3.10/site-packages (from datasets) (2.3.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/cc/miniconda3/envs/greenllm/lib/python3.10/site-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/cc/miniconda3/envs/greenllm/lib/python3.10/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /home/cc/miniconda3/envs/greenllm/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/cc/miniconda3/envs/greenllm/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /home/cc/miniconda3/envs/greenllm/lib/python3.10/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2025.9.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /home/cc/miniconda3/envs/greenllm/lib/python3.10/site-packages (from datasets) (0.35.3)\n",
      "Requirement already satisfied: packaging in /home/cc/miniconda3/envs/greenllm/lib/python3.10/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/cc/miniconda3/envs/greenllm/lib/python3.10/site-packages (from datasets) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/cc/miniconda3/envs/greenllm/lib/python3.10/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.12.15)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/cc/miniconda3/envs/greenllm/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/cc/miniconda3/envs/greenllm/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/cc/miniconda3/envs/greenllm/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/cc/miniconda3/envs/greenllm/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/cc/miniconda3/envs/greenllm/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/cc/miniconda3/envs/greenllm/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/cc/miniconda3/envs/greenllm/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/cc/miniconda3/envs/greenllm/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /home/cc/miniconda3/envs/greenllm/lib/python3.10/site-packages (from multidict<7.0,>=4.5->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.0 in /home/cc/miniconda3/envs/greenllm/lib/python3.10/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/cc/miniconda3/envs/greenllm/lib/python3.10/site-packages (from huggingface-hub>=0.24.0->datasets) (1.1.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/cc/miniconda3/envs/greenllm/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/cc/miniconda3/envs/greenllm/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/cc/miniconda3/envs/greenllm/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2025.8.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/cc/miniconda3/envs/greenllm/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/cc/miniconda3/envs/greenllm/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/cc/miniconda3/envs/greenllm/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/cc/miniconda3/envs/greenllm/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d27175bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/miniconda3/envs/greenllm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading EffiBench dataset...\n",
      "Dataset size: 1000\n",
      "Dataset features: ['problem_idx', 'task_name', 'description', 'markdown_description', 'canonical_solution', 'test_case_generator', 'test_case']\n",
      "DataFrame shape: (1000, 7)\n",
      "\n",
      "Column names:\n",
      "  - problem_idx\n",
      "  - task_name\n",
      "  - description\n",
      "  - markdown_description\n",
      "  - canonical_solution\n",
      "  - test_case_generator\n",
      "  - test_case\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Load the EffiBench Dataset\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load the EffiBench dataset\n",
    "print(\"Loading EffiBench dataset...\")\n",
    "dataset = load_dataset(\"DONG19/EffiBench\", split=\"train\")\n",
    "\n",
    "# Basic dataset info\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "print(f\"Dataset features: {list(dataset.features.keys())}\")\n",
    "\n",
    "# Convert to pandas for easier exploration\n",
    "df = dataset.to_pandas()\n",
    "print(f\"DataFrame shape: {df.shape}\")\n",
    "print(\"\\nColumn names:\")\n",
    "for col in df.columns:\n",
    "    print(f\"  - {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4edfd567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SAMPLE PROBLEM EXAMINATION ===\n",
      "\n",
      "--- Problem 0: Longest Substring Without Repeating Characters ---\n",
      "Description: \n",
      "\n",
      "<p>Given a string <code>s</code>, find the length of the <strong>longest</strong> <span data-keyword=\"substring-nonempty\"><strong>substring</strong></span> without repeating characters.</p>\n",
      "\n",
      "<p>&nbs...\n",
      "\n",
      "--- Canonical Solution ---\n",
      "class Solution:\n",
      "    def lengthOfLongestSubstring(self, s: str) -> int:\n",
      "        ss = set()\n",
      "        i = ans = 0\n",
      "        for j, c in enumerate(s):\n",
      "            while c in ss:\n",
      "                ss.remove(s[i])\n",
      "                i += 1\n",
      "            ss.add(c)\n",
      "            ans = max(ans, j - i + 1)\n",
      "        return...\n",
      "\n",
      "--- Test Case Generator ---\n",
      "\n",
      "import random\n",
      "\n",
      "class Solution:\n",
      "    def lengthOfLongestSubstring(self, s: str) -> int:\n",
      "        ss = set()\n",
      "        i = ans = 0\n",
      "        for j, c in enumerate(s):\n",
      "            while c in ss:\n",
      "                ss.remove(s[i])\n",
      "                i += 1\n",
      "            ss.add(c)\n",
      "            ans = max(ans, j - i + 1...\n",
      "\n",
      "--- Test Cases ---\n",
      "Type: <class 'str'>\n",
      "Content preview: assert solution.lengthOfLongestSubstring('') == 0\n",
      "assert solution.lengthOfLongestSubstring('krLKl6F') == 7\n",
      "assert solution.lengthOfLongestSubstring('p2Cn3Y6') == 7\n",
      "assert solution.lengthOfLongestSubstring('jf') == 2\n",
      "assert solution.lengthOfLongestSubstring('ebl') == 3\n",
      "assert solution.lengthOfLongestSubstring('7FHbLe') == 6\n",
      "assert solution.lengthOfLongestSubstring('cUoD0S') == 6\n",
      "assert solution.lengthOfLongestSubstring('M1kCixrcvS') == 10\n",
      "assert solution.lengthOfLongestSubstring('V9sGI') == 5\n",
      "ass...\n",
      "JSON parsing failed: Expecting value: line 1 column 1 (char 0)\n",
      "\n",
      "=== CHECKING TEST CASE FORMATS ACROSS SAMPLES ===\n",
      "Problem 0 (Longest Substring Without Repeating Characters): <class 'str'> - assert solution.lengthOfLongestSubstring('') == 0\n",
      "assert solution.lengthOfLongestSubstring('krLKl6F'...\n",
      "Problem 1 (Median of Two Sorted Arrays): <class 'str'> - assert solution.findMedianSortedArrays([20, 67], [37, 85]) == 52.0\n",
      "assert solution.findMedianSortedA...\n",
      "Problem 2 (Regular Expression Matching): <class 'str'> - assert solution.isMatch('xneafi', '.asrgzjwjjxxoho') == False\n",
      "assert solution.isMatch('ggmxwwkbeboui...\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Explore Test Case Format\n",
    "import json\n",
    "\n",
    "# Look at a few sample problems\n",
    "print(\"=== SAMPLE PROBLEM EXAMINATION ===\")\n",
    "sample_idx = 0\n",
    "\n",
    "print(f\"\\n--- Problem {sample_idx}: {df.iloc[sample_idx]['task_name']} ---\")\n",
    "print(f\"Description: {df.iloc[sample_idx]['description'][:200]}...\")\n",
    "\n",
    "print(\"\\n--- Canonical Solution ---\")\n",
    "print(df.iloc[sample_idx]['canonical_solution'][:300] + \"...\")\n",
    "\n",
    "print(\"\\n--- Test Case Generator ---\")\n",
    "print(df.iloc[sample_idx]['test_case_generator'][:300] + \"...\")\n",
    "\n",
    "print(\"\\n--- Test Cases ---\")\n",
    "test_case_raw = df.iloc[sample_idx]['test_case']\n",
    "print(f\"Type: {type(test_case_raw)}\")\n",
    "print(f\"Content preview: {str(test_case_raw)[:500]}...\")\n",
    "\n",
    "# Try to parse test cases if they're JSON\n",
    "try:\n",
    "    if isinstance(test_case_raw, str):\n",
    "        test_cases_parsed = json.loads(test_case_raw)\n",
    "        print(f\"\\nParsed as JSON - Type: {type(test_cases_parsed)}\")\n",
    "        if isinstance(test_cases_parsed, list):\n",
    "            print(f\"Number of test cases: {len(test_cases_parsed)}\")\n",
    "            if len(test_cases_parsed) > 0:\n",
    "                print(f\"First test case: {test_cases_parsed[0]}\")\n",
    "                print(f\"First test case keys: {list(test_cases_parsed[0].keys()) if isinstance(test_cases_parsed[0], dict) else 'Not a dict'}\")\n",
    "except Exception as e:\n",
    "    print(f\"JSON parsing failed: {e}\")\n",
    "    \n",
    "# Let's also check a few more samples\n",
    "print(f\"\\n=== CHECKING TEST CASE FORMATS ACROSS SAMPLES ===\")\n",
    "for i in range(min(3, len(df))):\n",
    "    test_case = df.iloc[i]['test_case']\n",
    "    print(f\"Problem {i} ({df.iloc[i]['task_name']}): {type(test_case)} - {str(test_case)[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "711a4164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PARSING TEST CASES ===\n",
      "\n",
      "--- Problem 0: Longest Substring Without Repeating Characters ---\n",
      "Parsed 100 test cases:\n",
      "  t1: inputs=, expected=0\n",
      "  t2: inputs=krLKl6F, expected=7\n",
      "  t3: inputs=p2Cn3Y6, expected=7\n",
      "  ... and 97 more test cases\n",
      "\n",
      "--- Problem 1: Median of Two Sorted Arrays ---\n",
      "Parsed 100 test cases:\n",
      "  t1: inputs=[[20, 67], [37, 85]], expected=52.0\n",
      "  t2: inputs=[[1, 2, 13, 22, 34, 46, 63, 86], [59, 80]], expected=40.0\n",
      "  t3: inputs=[[8, 57, 82, 87], [8, 18, 20, 23, 40, 41, 54, 63, 72, 93]], expected=47.5\n",
      "  ... and 97 more test cases\n",
      "\n",
      "--- Problem 2: Regular Expression Matching ---\n",
      "Parsed 100 test cases:\n",
      "  t1: inputs=['xneafi', '.asrgzjwjjxxoho'], expected=False\n",
      "  t2: inputs=['ggmxwwkbebouidkhdya', 'rhe'], expected=False\n",
      "  t3: inputs=['dyz', 'biragpervxyvwagor'], expected=False\n",
      "  ... and 97 more test cases\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Parse Assert Statements to Extract Test Data\n",
    "import re\n",
    "import ast\n",
    "\n",
    "def parse_assert_statements(test_case_string):\n",
    "    \"\"\"\n",
    "    Parse assert statements to extract inputs and expected outputs\n",
    "    Format: assert solution.method_name(args) == expected_output\n",
    "    \"\"\"\n",
    "    test_cases = []\n",
    "    \n",
    "    # Split into individual assert statements\n",
    "    assert_lines = [line.strip() for line in test_case_string.split('\\n') if line.strip().startswith('assert')]\n",
    "    \n",
    "    for i, line in enumerate(assert_lines):\n",
    "        try:\n",
    "            # Use regex to extract method call and expected output\n",
    "            # Pattern: assert solution.method_name(args) == expected_output\n",
    "            pattern = r'assert solution\\.(\\w+)\\((.*?)\\) == (.+)'\n",
    "            match = re.search(pattern, line)\n",
    "            \n",
    "            if match:\n",
    "                method_name = match.group(1)\n",
    "                args_str = match.group(2)\n",
    "                expected_output_str = match.group(3)\n",
    "                \n",
    "                # Parse arguments using ast.literal_eval for safety\n",
    "                try:\n",
    "                    if args_str.strip() == '':\n",
    "                        inputs = []\n",
    "                    else:\n",
    "                        # Try to parse as tuple of arguments\n",
    "                        inputs = ast.literal_eval(f\"({args_str},)\" if ',' not in args_str else f\"({args_str})\")\n",
    "                        if isinstance(inputs, tuple) and len(inputs) == 1:\n",
    "                            inputs = inputs[0]  # Single argument\n",
    "                        else:\n",
    "                            inputs = list(inputs)  # Multiple arguments\n",
    "                except:\n",
    "                    # If literal_eval fails, treat as single string argument\n",
    "                    inputs = args_str.strip(\"'\\\"\")\n",
    "                \n",
    "                # Parse expected output\n",
    "                try:\n",
    "                    expected_output = ast.literal_eval(expected_output_str)\n",
    "                except:\n",
    "                    expected_output = expected_output_str.strip(\"'\\\"\")\n",
    "                \n",
    "                test_cases.append({\n",
    "                    'test_id': f't{i+1}',\n",
    "                    'method_name': method_name,\n",
    "                    'inputs': inputs,\n",
    "                    'expected_output': expected_output,\n",
    "                    'original_line': line\n",
    "                })\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Failed to parse line: {line}\")\n",
    "            print(f\"Error: {e}\")\n",
    "    \n",
    "    return test_cases\n",
    "\n",
    "# Test the parser on sample problems\n",
    "print(\"=== PARSING TEST CASES ===\")\n",
    "for idx in range(min(3, len(df))):\n",
    "    problem_name = df.iloc[idx]['task_name']\n",
    "    test_case_str = df.iloc[idx]['test_case']\n",
    "    \n",
    "    print(f\"\\n--- Problem {idx}: {problem_name} ---\")\n",
    "    parsed_cases = parse_assert_statements(test_case_str)\n",
    "    \n",
    "    print(f\"Parsed {len(parsed_cases)} test cases:\")\n",
    "    for case in parsed_cases[:3]:  # Show first 3\n",
    "        print(f\"  {case['test_id']}: inputs={case['inputs']}, expected={case['expected_output']}\")\n",
    "    \n",
    "    if len(parsed_cases) > 3:\n",
    "        print(f\"  ... and {len(parsed_cases) - 3} more test cases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f827ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PREPARING SOLUTIONS FOR JOULETRACE ===\n",
      "\n",
      "--- Problem 0: Longest Substring Without Repeating Characters ---\n",
      "Detected function name: lengthOfLongestSubstring\n",
      "Wrapper function created:\n",
      "# Original solution\n",
      "class Solution:\n",
      "    def lengthOfLongestSubstring(self, s: str) -> int:\n",
      "        ss = set()\n",
      "        i = ans = 0\n",
      "        for j, c in enumerate(s):\n",
      "            while c in ss:\n",
      "         ...\n",
      "Test case method name: lengthOfLongestSubstring\n",
      "Names match: True\n",
      "\n",
      "--- Problem 1: Median of Two Sorted Arrays ---\n",
      "Detected function name: findMedianSortedArrays\n",
      "Wrapper function created:\n",
      "# Original solution\n",
      "class Solution:\n",
      "    def findMedianSortedArrays(self, nums1: List[int], nums2: List[int]) -> float:\n",
      "        def f(i: int, j: int, k: int) -> int:\n",
      "            if i >= m:\n",
      "            ...\n",
      "Test case method name: findMedianSortedArrays\n",
      "Names match: True\n",
      "\n",
      "--- Problem 2: Regular Expression Matching ---\n",
      "Detected function name: isMatch\n",
      "Wrapper function created:\n",
      "# Original solution\n",
      "class Solution:\n",
      "    def isMatch(self, s: str, p: str) -> bool:\n",
      "        m, n = len(s), len(p)\n",
      "        f = [[False] * (n + 1) for _ in range(m + 1)]\n",
      "        f[0][0] = True\n",
      "        fo...\n",
      "Test case method name: isMatch\n",
      "Names match: True\n",
      "\n",
      "=== FUNCTION NAME ANALYSIS ===\n",
      "Total problems with extractable function names: 1000/1000\n",
      "Most common function names:\n",
      "  minOperations: 10\n",
      "  insert: 8\n",
      "  minCost: 8\n",
      "  minimumTime: 6\n",
      "  minimumOperations: 6\n",
      "  maxProfit: 5\n",
      "  lowbit: 5\n",
      "  lowestCommonAncestor: 4\n",
      "  update: 4\n",
      "  countSubarrays: 4\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Extract Function Names and Prepare Solutions\n",
    "import re\n",
    "\n",
    "def extract_function_name_from_solution(canonical_solution):\n",
    "    \"\"\"Extract the main function name from the canonical solution\"\"\"\n",
    "    # Look for class methods first\n",
    "    class_method_pattern = r'def\\s+(\\w+)\\s*\\('\n",
    "    matches = re.findall(class_method_pattern, canonical_solution)\n",
    "    \n",
    "    if matches:\n",
    "        # Filter out __init__ and other special methods\n",
    "        methods = [m for m in matches if not m.startswith('__')]\n",
    "        if methods:\n",
    "            return methods[0]  # Return first non-special method\n",
    "    \n",
    "    return None\n",
    "\n",
    "def create_wrapper_function(canonical_solution, function_name):\n",
    "    \"\"\"\n",
    "    Create a top-level wrapper function for jouletrace.\n",
    "    Jouletrace needs a top-level function, but EffiBench has class methods.\n",
    "    \"\"\"\n",
    "    wrapper_code = f\"\"\"# Original solution\n",
    "{canonical_solution}\n",
    "\n",
    "# Wrapper function for jouletrace\n",
    "def solve(*args, **kwargs):\n",
    "    solution = Solution()\n",
    "    return solution.{function_name}(*args, **kwargs)\n",
    "\"\"\"\n",
    "    return wrapper_code\n",
    "\n",
    "# Test solution preparation\n",
    "print(\"=== PREPARING SOLUTIONS FOR JOULETRACE ===\")\n",
    "\n",
    "for idx in range(min(3, len(df))):\n",
    "    problem_name = df.iloc[idx]['task_name']\n",
    "    canonical_solution = df.iloc[idx]['canonical_solution']\n",
    "    \n",
    "    print(f\"\\n--- Problem {idx}: {problem_name} ---\")\n",
    "    \n",
    "    # Extract function name\n",
    "    func_name = extract_function_name_from_solution(canonical_solution)\n",
    "    print(f\"Detected function name: {func_name}\")\n",
    "    \n",
    "    if func_name:\n",
    "        # Create wrapper\n",
    "        wrapped_solution = create_wrapper_function(canonical_solution, func_name)\n",
    "        print(\"Wrapper function created:\")\n",
    "        print(wrapped_solution[:200] + \"...\")\n",
    "        \n",
    "        # Also show what the test case parsing found for method name\n",
    "        test_case_str = df.iloc[idx]['test_case']\n",
    "        parsed_cases = parse_assert_statements(test_case_str)\n",
    "        if parsed_cases:\n",
    "            test_method_name = parsed_cases[0]['method_name']\n",
    "            print(f\"Test case method name: {test_method_name}\")\n",
    "            print(f\"Names match: {func_name == test_method_name}\")\n",
    "    else:\n",
    "        print(\"Could not extract function name\")\n",
    "\n",
    "# Let's also check the distribution of function names across the dataset\n",
    "print(f\"\\n=== FUNCTION NAME ANALYSIS ===\")\n",
    "function_names = []\n",
    "for idx in range(len(df)):\n",
    "    canonical_solution = df.iloc[idx]['canonical_solution']\n",
    "    func_name = extract_function_name_from_solution(canonical_solution)\n",
    "    if func_name:\n",
    "        function_names.append(func_name)\n",
    "\n",
    "from collections import Counter\n",
    "name_counts = Counter(function_names)\n",
    "print(f\"Total problems with extractable function names: {len(function_names)}/{len(df)}\")\n",
    "print(\"Most common function names:\")\n",
    "for name, count in name_counts.most_common(10):\n",
    "    print(f\"  {name}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13464d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BUILDING JOULETRACE REQUESTS ===\n",
      "Building request for problem 0: Longest Substring Without Repeating Characters\n",
      "\n",
      "Request structure:\n",
      "- Function name: solve\n",
      "- Number of test cases: 5\n",
      "- Candidate ID: effibench_0\n",
      "- Problem name: Longest Substring Without Repeating Characters\n",
      "\n",
      "Sample test cases:\n",
      "  t1: inputs=[''], expected=0\n",
      "  t2: inputs=['krLKl6F'], expected=7\n",
      "  t3: inputs=['p2Cn3Y6'], expected=7\n",
      "\n",
      "Code preview:\n",
      "# Original solution\n",
      "class Solution:\n",
      "    def lengthOfLongestSubstring(self, s: str) -> int:\n",
      "        ss = set()\n",
      "        i = ans = 0\n",
      "        for j, c in enumerate(s):\n",
      "            while c in ss:\n",
      "                ss.remove(s[i])\n",
      "                i += 1\n",
      "            ss.add(c)\n",
      "            ans = max(ans, j - i...\n",
      "\n",
      "Full request JSON (first 1000 chars):\n",
      "{\n",
      "  \"candidate_code\": \"# Original solution\\nclass Solution:\\n    def lengthOfLongestSubstring(self, s: str) -> int:\\n        ss = set()\\n        i = ans = 0\\n        for j, c in enumerate(s):\\n            while c in ss:\\n                ss.remove(s[i])\\n                i += 1\\n            ss.add(c)\\n            ans = max(ans, j - i + 1)\\n        return ans\\n\\n\\n# Wrapper function for jouletrace\\ndef solve(*args, **kwargs):\\n    solution = Solution()\\n    return solution.lengthOfLongestSubstring(*args, **kwargs)\\n\",\n",
      "  \"function_name\": \"solve\",\n",
      "  \"test_cases\": [\n",
      "    {\n",
      "      \"test_id\": \"t1\",\n",
      "      \"inputs\": [\n",
      "        \"\"\n",
      "      ],\n",
      "      \"expected_output\": 0\n",
      "    },\n",
      "    {\n",
      "      \"test_id\": \"t2\",\n",
      "      \"inputs\": [\n",
      "        \"krLKl6F\"\n",
      "      ],\n",
      "      \"expected_output\": 7\n",
      "    },\n",
      "    {\n",
      "      \"test_id\": \"t3\",\n",
      "      \"inputs\": [\n",
      "        \"p2Cn3Y6\"\n",
      "      ],\n",
      "      \"expected_output\": 7\n",
      "    },\n",
      "    {\n",
      "      \"test_id\": \"t4\",\n",
      "      \"inputs\": [\n",
      "        \"jf\"\n",
      "      ],\n",
      "      \"expected_output\": 2\n",
      "    },\n",
      "    {\n",
      "      \"...\n",
      "\n",
      "=== TESTING ON MULTIPLE PROBLEMS ===\n",
      "✓ Problem 1 (Median of Two Sorted Arrays): 3 test cases\n",
      "✓ Problem 2 (Regular Expression Matching): 3 test cases\n",
      "✓ Problem 3 (Container With Most Water): 3 test cases\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Build JouletTrace API Request\n",
    "import json\n",
    "\n",
    "def build_jouletrace_request(problem_idx, df, max_test_cases=None):\n",
    "    \"\"\"\n",
    "    Convert an EffiBench problem into a jouletrace API request format\n",
    "    \"\"\"\n",
    "    problem_data = df.iloc[problem_idx]\n",
    "    \n",
    "    # Extract basic info\n",
    "    task_name = problem_data['task_name']\n",
    "    canonical_solution = problem_data['canonical_solution']\n",
    "    test_case_str = problem_data['test_case']\n",
    "    \n",
    "    # Get function name and create wrapper\n",
    "    func_name = extract_function_name_from_solution(canonical_solution)\n",
    "    if not func_name:\n",
    "        raise ValueError(f\"Could not extract function name for problem {problem_idx}\")\n",
    "    \n",
    "    wrapped_solution = create_wrapper_function(canonical_solution, func_name)\n",
    "    \n",
    "    # Parse test cases\n",
    "    parsed_test_cases = parse_assert_statements(test_case_str)\n",
    "    \n",
    "    # Limit test cases if specified\n",
    "    if max_test_cases:\n",
    "        parsed_test_cases = parsed_test_cases[:max_test_cases]\n",
    "    \n",
    "    # Convert to jouletrace format\n",
    "    jouletrace_test_cases = []\n",
    "    for case in parsed_test_cases:\n",
    "        # Determine input format based on jouletrace call semantics\n",
    "        inputs = case['inputs']\n",
    "        \n",
    "        # Handle different input types for jouletrace\n",
    "        if isinstance(inputs, list):\n",
    "            # Multiple positional args: func(*inputs)\n",
    "            jouletrace_inputs = inputs\n",
    "        elif isinstance(inputs, dict):\n",
    "            # Keyword args: func(**inputs)  \n",
    "            jouletrace_inputs = inputs\n",
    "        else:\n",
    "            # Single positional arg: func(inputs)\n",
    "            jouletrace_inputs = [inputs]\n",
    "        \n",
    "        jouletrace_test_cases.append({\n",
    "            \"test_id\": case['test_id'],\n",
    "            \"inputs\": jouletrace_inputs,\n",
    "            \"expected_output\": case['expected_output']\n",
    "        })\n",
    "    \n",
    "    # Build the complete request\n",
    "    request = {\n",
    "        \"candidate_code\": wrapped_solution,\n",
    "        \"function_name\": \"solve\",  # Our wrapper function name\n",
    "        \"test_cases\": jouletrace_test_cases,\n",
    "        \"timeout_seconds\": 30,  # Generous timeout for efficiency problems\n",
    "        \"memory_limit_mb\": 512,  # Reasonable memory limit\n",
    "        \"energy_measurement_trials\": 5,\n",
    "        \"warmup_trials\": 2,\n",
    "        \"candidate_id\": f\"effibench_{problem_idx}\",\n",
    "        \"problem_name\": task_name\n",
    "    }\n",
    "    \n",
    "    return request\n",
    "\n",
    "# Test the request builder on a sample problem\n",
    "print(\"=== BUILDING JOULETRACE REQUESTS ===\")\n",
    "\n",
    "sample_idx = 0\n",
    "print(f\"Building request for problem {sample_idx}: {df.iloc[sample_idx]['task_name']}\")\n",
    "\n",
    "try:\n",
    "    request = build_jouletrace_request(sample_idx, df, max_test_cases=5)\n",
    "    \n",
    "    print(f\"\\nRequest structure:\")\n",
    "    print(f\"- Function name: {request['function_name']}\")\n",
    "    print(f\"- Number of test cases: {len(request['test_cases'])}\")\n",
    "    print(f\"- Candidate ID: {request['candidate_id']}\")\n",
    "    print(f\"- Problem name: {request['problem_name']}\")\n",
    "    \n",
    "    print(f\"\\nSample test cases:\")\n",
    "    for i, test_case in enumerate(request['test_cases'][:3]):\n",
    "        print(f\"  {test_case['test_id']}: inputs={test_case['inputs']}, expected={test_case['expected_output']}\")\n",
    "    \n",
    "    print(f\"\\nCode preview:\")\n",
    "    print(request['candidate_code'][:300] + \"...\")\n",
    "    \n",
    "    print(f\"\\nFull request JSON (first 1000 chars):\")\n",
    "    request_json = json.dumps(request, indent=2)\n",
    "    print(request_json[:1000] + \"...\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error building request: {e}\")\n",
    "\n",
    "# Test on a few more problems to ensure consistency\n",
    "print(f\"\\n=== TESTING ON MULTIPLE PROBLEMS ===\")\n",
    "for idx in range(1, min(4, len(df))):\n",
    "    try:\n",
    "        request = build_jouletrace_request(idx, df, max_test_cases=3)\n",
    "        print(f\"✓ Problem {idx} ({df.iloc[idx]['task_name']}): {len(request['test_cases'])} test cases\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Problem {idx}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7b70123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TESTING JOULETRACE API INTEGRATION ===\n",
      "Preparing to test problem 0: Longest Substring Without Repeating Characters\n",
      "Request ready:\n",
      "- Problem: Longest Substring Without Repeating Characters\n",
      "- Test cases: 3\n",
      "- Code length: 477 characters\n",
      "\\nSending request to JouletTrace...\n",
      "Task queued successfully!\n",
      "Task ID: f2845a57-1cac-4605-bdac-737f5cc944ce\n",
      "Estimated completion: 93 seconds\n",
      "Poll URL: /api/v1/tasks/f2845a57-1cac-4605-bdac-737f5cc944ce\n",
      "\\nPolling for results...\n",
      "Poll 1: Status = running\n",
      "  Running: validation (10.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poll 2: Status = completed\n",
      "\\nFinal result:\n",
      "Status: completed\n",
      "Correctness: True\n",
      "Passed tests: 3/3\n",
      "\\nEnergy Metrics:\n",
      "- Total energy: 1.54 joules\n",
      "- Execution time: 1.3491997378878295e-05 seconds\n",
      "- Power consumption: 114141.73578264017 watts\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Send Request to JouletTrace API\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "from typing import Dict, Any\n",
    "\n",
    "# Configure your JouletTrace API endpoint\n",
    "JOULETRACE_BASE_URL = \"http://localhost:8000\"  # Replace with actual endpoint\n",
    "# If you need authentication, add headers here\n",
    "API_HEADERS = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    # \"Authorization\": \"Bearer your-token-here\"  # Add if needed\n",
    "}\n",
    "\n",
    "def send_jouletrace_request(request_data: Dict[Any, Any]) -> Dict[Any, Any]:\n",
    "    \"\"\"\n",
    "    Send a request to the JouletTrace API\n",
    "    Returns the task queued response\n",
    "    \"\"\"\n",
    "    url = f\"{JOULETRACE_BASE_URL}/api/v1/measure\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(url, json=request_data, headers=API_HEADERS)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Request failed: {e}\")\n",
    "        if hasattr(e, 'response') and e.response is not None:\n",
    "            print(f\"Response: {e.response.text}\")\n",
    "        raise\n",
    "\n",
    "def poll_task_result(task_id: str, max_polls: int = 60, poll_interval: int = 5) -> Dict[Any, Any]:\n",
    "    \"\"\"\n",
    "    Poll the JouletTrace API for task completion\n",
    "    Returns the final result\n",
    "    \"\"\"\n",
    "    url = f\"{JOULETRACE_BASE_URL}/api/v1/tasks/{task_id}\"\n",
    "    \n",
    "    for attempt in range(max_polls):\n",
    "        try:\n",
    "            response = requests.get(url, headers=API_HEADERS)\n",
    "            response.raise_for_status()\n",
    "            result = response.json()\n",
    "            \n",
    "            status = result.get('status')\n",
    "            print(f\"Poll {attempt + 1}: Status = {status}\")\n",
    "            \n",
    "            if status == \"completed\":\n",
    "                return result\n",
    "            elif status == \"failed\":\n",
    "                print(f\"Task failed: {result.get('error_message', 'Unknown error')}\")\n",
    "                return result\n",
    "            elif status in [\"queued\", \"running\"]:\n",
    "                if status == \"running\":\n",
    "                    stage = result.get('stage', 'Unknown')\n",
    "                    progress = result.get('progress', 0)\n",
    "                    print(f\"  Running: {stage} ({progress*100:.1f}%)\")\n",
    "                time.sleep(poll_interval)\n",
    "            else:\n",
    "                print(f\"Unknown status: {status}\")\n",
    "                return result\n",
    "                \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Polling failed: {e}\")\n",
    "            time.sleep(poll_interval)\n",
    "    \n",
    "    raise TimeoutError(f\"Task {task_id} did not complete within {max_polls * poll_interval} seconds\")\n",
    "\n",
    "# Test with a single problem (DRY RUN - comment out the actual API calls for now)\n",
    "print(\"=== TESTING JOULETRACE API INTEGRATION ===\")\n",
    "\n",
    "# Build a request for the first problem\n",
    "sample_idx = 0\n",
    "print(f\"Preparing to test problem {sample_idx}: {df.iloc[sample_idx]['task_name']}\")\n",
    "\n",
    "try:\n",
    "    request = build_jouletrace_request(sample_idx, df, max_test_cases=3)  # Limit to 3 test cases for testing\n",
    "    \n",
    "    print(f\"Request ready:\")\n",
    "    print(f\"- Problem: {request['problem_name']}\")\n",
    "    print(f\"- Test cases: {len(request['test_cases'])}\")\n",
    "    print(f\"- Code length: {len(request['candidate_code'])} characters\")\n",
    "    \n",
    "    # UNCOMMENT THESE LINES WHEN YOU'RE READY TO TEST WITH REAL API:\n",
    "    print(\"\\\\nSending request to JouletTrace...\")\n",
    "    queued_response = send_jouletrace_request(request)\n",
    "    \n",
    "    print(f\"Task queued successfully!\")\n",
    "    print(f\"Task ID: {queued_response['task_id']}\")\n",
    "    print(f\"Estimated completion: {queued_response.get('estimated_completion_seconds', 'Unknown')} seconds\")\n",
    "    print(f\"Poll URL: {queued_response.get('poll_url', 'N/A')}\")\n",
    "    \n",
    "    print(\"\\\\nPolling for results...\")\n",
    "    final_result = poll_task_result(queued_response['task_id'])\n",
    "    \n",
    "    print(f\"\\\\nFinal result:\")\n",
    "    print(f\"Status: {final_result['status']}\")\n",
    "    \n",
    "    if final_result['status'] == 'completed':\n",
    "        validation = final_result.get('validation', {})\n",
    "        print(f\"Correctness: {validation.get('is_correct', 'Unknown')}\")\n",
    "        print(f\"Passed tests: {validation.get('passed_tests', 0)}/{validation.get('total_tests', 0)}\")\n",
    "        \n",
    "        energy_metrics = final_result.get('energy_metrics', {})\n",
    "        if energy_metrics:\n",
    "            print(f\"\\\\nEnergy Metrics:\")\n",
    "            print(f\"- Total energy: {energy_metrics.get('median_total_energy_joules', 'N/A')} joules\")\n",
    "            print(f\"- Execution time: {energy_metrics.get('median_execution_time_seconds', 'N/A')} seconds\")\n",
    "            print(f\"- Power consumption: {energy_metrics.get('power_consumption_watts', 'N/A')} watts\")\n",
    "    \n",
    "    # print(f\"\\nTO ACTUALLY RUN:\")\n",
    "    # print(f\"1. Replace JOULETRACE_BASE_URL with your actual API endpoint\")\n",
    "    # print(f\"2. Add authentication headers if needed\")\n",
    "    # print(f\"3. Uncomment the API call lines above\")\n",
    "    # print(f\"4. Run this cell again\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error in test setup: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d1546cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'service': 'jouletrace'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import requests\n",
    "\n",
    "requests.get(f\"{JOULETRACE_BASE_URL}/ping\").json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edcfe6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "from urllib.parse import urljoin\n",
    "from pprint import pprint\n",
    "\n",
    "# Set once per notebook (falls back to localhost if not set)\n",
    "JOULETRACE_BASE_URL = os.getenv(\"JOULETRACE_BASE_URL\", \"http://127.0.0.1:8000\")\n",
    "\n",
    "def poll_from_poll_url(poll_url: str, timeout_s: int = 600, interval_s: float = 0.5):\n",
    "    \"\"\"\n",
    "    Poll a JouleTrace task until it finishes (completed/failed).\n",
    "    Accepts either a relative poll_url (e.g., /api/v1/tasks/<id>)\n",
    "    or an absolute URL.\n",
    "    \"\"\"\n",
    "    # Build full URL if needed\n",
    "    url = poll_url if poll_url.startswith(\"http\") else urljoin(JOULETRACE_BASE_URL, poll_url)\n",
    "    s = requests.Session()\n",
    "    start = time.time()\n",
    "    last_status = None\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            r = s.get(url, timeout=30)\n",
    "            r.raise_for_status()\n",
    "            data = r.json()\n",
    "            status = data.get(\"status\")\n",
    "            # Print status transitions or occasional updates\n",
    "            if status != last_status or status == \"running\":\n",
    "                if status == \"running\":\n",
    "                    stage = data.get(\"stage\") or data.get(\"meta\", {}).get(\"stage\")\n",
    "                    progress = data.get(\"progress\")\n",
    "                    print(f\"status=running stage={stage} progress={progress}\")\n",
    "                else:\n",
    "                    print(f\"status={status}\")\n",
    "                last_status = status\n",
    "\n",
    "            if status in {\"completed\", \"failed\"}:\n",
    "                return data\n",
    "\n",
    "            if time.time() - start > timeout_s:\n",
    "                raise TimeoutError(f\"Polling timed out after {timeout_s}s: {url}\")\n",
    "\n",
    "            time.sleep(interval_s)\n",
    "\n",
    "        except requests.RequestException as e:\n",
    "            # Brief backoff on transient errors\n",
    "            print(f\"poll error: {e}; retrying…\")\n",
    "            time.sleep(1.0)\n",
    "\n",
    "# Example usage:\n",
    "# queued = requests.post(f\"{JOULETRACE_BASE_URL}/api/v1/measure\", json=payload).json()\n",
    "# result = poll_from_poll_url(queued[\"poll_url\"])\n",
    "# pprint(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42bd1c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "status=completed\n"
     ]
    }
   ],
   "source": [
    "result = poll_from_poll_url(queued_response.get('poll_url', 'N/A'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f8fbc21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SETTING UP BATCH PROCESSING ===\n",
      "Test batch: problems [0, 1, 2, 3, 4]\n",
      "\n",
      "Problems to process:\n",
      "  0: Longest Substring Without Repeating Characters\n",
      "  1: Median of Two Sorted Arrays\n",
      "  2: Regular Expression Matching\n",
      "  3: Container With Most Water\n",
      "  4: Generate Parentheses\n",
      "\n",
      "✅ READY TO RUN BATCH PROCESSING!\n",
      "JouletTrace endpoint: http://127.0.0.1:8000\n",
      "To start processing:\n",
      "results = processor.process_batch(test_indices, max_test_cases_per_problem=5)\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Batch Processing Multiple Problems\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any\n",
    "import pandas as pd\n",
    "\n",
    "class EffiBenchJouletTraceProcessor:\n",
    "    def __init__(self, df: pd.DataFrame, base_url: str, headers: Dict[str, str] = None):\n",
    "        self.df = df\n",
    "        self.base_url = base_url\n",
    "        self.headers = headers or {\"Content-Type\": \"application/json\"}\n",
    "        self.results = {}\n",
    "        self.failed_problems = {}\n",
    "        \n",
    "    def process_batch(self, \n",
    "                     problem_indices: List[int], \n",
    "                     max_test_cases_per_problem: int = 10,\n",
    "                     batch_delay: float = 2.0,\n",
    "                     results_file: str = None) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Process a batch of EffiBench problems through JouletTrace\n",
    "        \"\"\"\n",
    "        if results_file is None:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            results_file = f\"effibench_jouletrace_results_{timestamp}.json\"\n",
    "        \n",
    "        # Load existing results if file exists\n",
    "        if os.path.exists(results_file):\n",
    "            with open(results_file, 'r') as f:\n",
    "                existing_data = json.load(f)\n",
    "                self.results = existing_data.get('results', {})\n",
    "                self.failed_problems = existing_data.get('failed_problems', {})\n",
    "            print(f\"Loaded existing results: {len(self.results)} completed, {len(self.failed_problems)} failed\")\n",
    "        \n",
    "        print(f\"Processing {len(problem_indices)} problems...\")\n",
    "        print(f\"Results will be saved to: {results_file}\")\n",
    "        \n",
    "        for i, prob_idx in enumerate(problem_indices):\n",
    "            prob_key = str(prob_idx)\n",
    "            \n",
    "            # Skip if already processed\n",
    "            if prob_key in self.results or prob_key in self.failed_problems:\n",
    "                print(f\"Skipping problem {prob_idx} (already processed)\")\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                print(f\"\\n[{i+1}/{len(problem_indices)}] Processing problem {prob_idx}: {self.df.iloc[prob_idx]['task_name']}\")\n",
    "                \n",
    "                # Build request\n",
    "                request = build_jouletrace_request(prob_idx, self.df, max_test_cases_per_problem)\n",
    "                \n",
    "                # Send request\n",
    "                queued_response = self._send_request(request)\n",
    "                task_id = queued_response['task_id']\n",
    "                \n",
    "                print(f\"  Task queued: {task_id}\")\n",
    "                \n",
    "                # Poll for result\n",
    "                result = self._poll_result(task_id)\n",
    "                \n",
    "                # Store result\n",
    "                self.results[prob_key] = {\n",
    "                    'problem_idx': prob_idx,\n",
    "                    'task_name': self.df.iloc[prob_idx]['task_name'],\n",
    "                    'task_id': task_id,\n",
    "                    'request_timestamp': datetime.now().isoformat(),\n",
    "                    'result': result\n",
    "                }\n",
    "                \n",
    "                # Log summary\n",
    "                if result['status'] == 'completed':\n",
    "                    validation = result.get('validation', {})\n",
    "                    energy = result.get('energy_metrics', {})\n",
    "                    print(f\"  ✓ Completed: {validation.get('passed_tests', 0)}/{validation.get('total_tests', 0)} tests passed\")\n",
    "                    if energy:\n",
    "                        print(f\"    Energy: {energy.get('median_total_energy_joules', 'N/A')} joules, {energy.get('median_execution_time_seconds', 'N/A')} seconds\")\n",
    "                else:\n",
    "                    print(f\"  ✗ Failed: {result.get('error_message', 'Unknown error')}\")\n",
    "                \n",
    "                print(f\"  ✓ Result stored\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  ✗ Error processing problem {prob_idx}: {e}\")\n",
    "                self.failed_problems[prob_key] = {\n",
    "                    'problem_idx': prob_idx,\n",
    "                    'task_name': self.df.iloc[prob_idx]['task_name'],\n",
    "                    'error': str(e),\n",
    "                    'timestamp': datetime.now().isoformat()\n",
    "                }\n",
    "            \n",
    "            # Save progress after each problem\n",
    "            self._save_results(results_file)\n",
    "            \n",
    "            # Rate limiting delay\n",
    "            if i < len(problem_indices) - 1:  # Don't wait after last item\n",
    "                time.sleep(batch_delay)\n",
    "        \n",
    "        print(f\"\\n🏁 Batch processing complete!\")\n",
    "        print(f\"✅ Successful: {len(self.results)}\")\n",
    "        print(f\"❌ Failed: {len(self.failed_problems)}\")\n",
    "        \n",
    "        return {\n",
    "            'results': self.results,\n",
    "            'failed_problems': self.failed_problems,\n",
    "            'results_file': results_file\n",
    "        }\n",
    "    \n",
    "    def _save_results(self, results_file: str):\n",
    "        \"\"\"Save current results to file\"\"\"\n",
    "        data = {\n",
    "            'results': self.results,\n",
    "            'failed_problems': self.failed_problems,\n",
    "            'metadata': {\n",
    "                'total_problems_processed': len(self.results) + len(self.failed_problems),\n",
    "                'successful_problems': len(self.results),\n",
    "                'failed_problems': len(self.failed_problems),\n",
    "                'last_updated': datetime.now().isoformat()\n",
    "            }\n",
    "        }\n",
    "        with open(results_file, 'w') as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "    \n",
    "    def _send_request(self, request_data: Dict[Any, Any]) -> Dict[Any, Any]:\n",
    "        \"\"\"Send a request to the JouletTrace API\"\"\"\n",
    "        url = f\"{self.base_url}/api/v1/measure\"\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(url, json=request_data, headers=self.headers)\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request failed: {e}\")\n",
    "            if hasattr(e, 'response') and e.response is not None:\n",
    "                print(f\"Response: {e.response.text}\")\n",
    "            raise\n",
    "    \n",
    "    def _poll_result(self, task_id: str, max_polls: int = 60, poll_interval: int = 5) -> Dict[Any, Any]:\n",
    "        \"\"\"Poll the JouletTrace API for task completion\"\"\"\n",
    "        url = f\"{self.base_url}/api/v1/tasks/{task_id}\"\n",
    "        \n",
    "        for attempt in range(max_polls):\n",
    "            try:\n",
    "                response = requests.get(url, headers=self.headers)\n",
    "                response.raise_for_status()\n",
    "                result = response.json()\n",
    "                \n",
    "                status = result.get('status')\n",
    "                if attempt % 10 == 0:  # Print every 10th poll to reduce noise\n",
    "                    print(f\"    Poll {attempt + 1}: Status = {status}\")\n",
    "                \n",
    "                if status == \"completed\":\n",
    "                    return result\n",
    "                elif status == \"failed\":\n",
    "                    print(f\"    Task failed: {result.get('error_message', 'Unknown error')}\")\n",
    "                    return result\n",
    "                elif status in [\"queued\", \"running\"]:\n",
    "                    if status == \"running\" and attempt % 5 == 0:  # Show progress occasionally\n",
    "                        stage = result.get('stage', 'Unknown')\n",
    "                        progress = result.get('progress', 0)\n",
    "                        print(f\"    Running: {stage} ({progress*100:.1f}%)\")\n",
    "                    time.sleep(poll_interval)\n",
    "                else:\n",
    "                    print(f\"    Unknown status: {status}\")\n",
    "                    return result\n",
    "                    \n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"    Polling failed: {e}\")\n",
    "                time.sleep(poll_interval)\n",
    "        \n",
    "        raise TimeoutError(f\"Task {task_id} did not complete within {max_polls * poll_interval} seconds\")\n",
    "\n",
    "# Test batch processing on a small subset\n",
    "print(\"=== SETTING UP BATCH PROCESSING ===\")\n",
    "\n",
    "# Initialize processor\n",
    "processor = EffiBenchJouletTraceProcessor(\n",
    "    df=df, \n",
    "    base_url=\"http://127.0.0.1:8000\",\n",
    "    headers={\"Content-Type\": \"application/json\"}\n",
    ")\n",
    "\n",
    "# Test on first 5 problems\n",
    "test_indices = list(range(5))\n",
    "print(f\"Test batch: problems {test_indices}\")\n",
    "\n",
    "# Show what we're about to process\n",
    "print(\"\\nProblems to process:\")\n",
    "for idx in test_indices:\n",
    "    print(f\"  {idx}: {df.iloc[idx]['task_name']}\")\n",
    "\n",
    "print(f\"\\n✅ READY TO RUN BATCH PROCESSING!\")\n",
    "print(f\"JouletTrace endpoint: http://127.0.0.1:8000\")\n",
    "print(f\"To start processing:\")\n",
    "print(f\"results = processor.process_batch(test_indices, max_test_cases_per_problem=5)\")\n",
    "\n",
    "# Ready to run - uncomment this line when you want to start:\n",
    "# results = processor.process_batch(test_indices, max_test_cases_per_problem=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b0457d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 5 problems...\n",
      "Results will be saved to: effibench_jouletrace_results_20250929_225137.json\n",
      "\n",
      "[1/5] Processing problem 0: Longest Substring Without Repeating Characters\n",
      "  Task queued: 2f02a702-030d-4d82-8713-258989d55f14\n",
      "    Poll 1: Status = running\n",
      "    Running: initializing (0.0%)\n",
      "  ✓ Completed: 5/5 tests passed\n",
      "    Energy: 1.1400000000000001 joules, 1.639200490899384e-05 seconds\n",
      "  ✓ Result stored\n",
      "\n",
      "[2/5] Processing problem 1: Median of Two Sorted Arrays\n",
      "  Task queued: 4909f4d3-d29c-4e00-8d1e-8d1bff4b920d\n",
      "    Poll 1: Status = running\n",
      "    Running: validation (10.0%)\n",
      "    Task failed: Energy measurement failed\n",
      "  ✗ Failed: Energy measurement failed\n",
      "  ✓ Result stored\n",
      "\n",
      "[3/5] Processing problem 2: Regular Expression Matching\n",
      "  Task queued: 0b6376fa-1742-4b0f-a290-3af1ac83cd00\n",
      "    Poll 1: Status = running\n",
      "    Running: validation (10.0%)\n",
      "  ✓ Completed: 5/5 tests passed\n",
      "    Energy: 1.47 joules, 7.670599734410644e-05 seconds\n",
      "  ✓ Result stored\n",
      "\n",
      "[4/5] Processing problem 3: Container With Most Water\n",
      "  Task queued: 945a10c4-4b25-452f-9c83-76548a3e4947\n",
      "    Poll 1: Status = running\n",
      "    Running: validation (10.0%)\n",
      "    Task failed: Energy measurement failed\n",
      "  ✗ Failed: Energy measurement failed\n",
      "  ✓ Result stored\n",
      "\n",
      "[5/5] Processing problem 4: Generate Parentheses\n",
      "  Task queued: a2160aa1-d67e-4a90-963b-27779ae92950\n",
      "    Poll 1: Status = running\n",
      "    Running: initializing (0.0%)\n",
      "  ✓ Completed: 5/5 tests passed\n",
      "    Energy: 1.47 joules, 0.0001440049964003265 seconds\n",
      "  ✓ Result stored\n",
      "\n",
      "🏁 Batch processing complete!\n",
      "✅ Successful: 5\n",
      "❌ Failed: 0\n"
     ]
    }
   ],
   "source": [
    "results = processor.process_batch(test_indices, max_test_cases_per_problem=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3de85b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_wrapper_code(class_src: str, class_name: str, method_name: str,\n",
    "                       public_name: str, arg_names):\n",
    "    \"\"\"\n",
    "    Compose candidate_code that defines your class AND a top-level wrapper\n",
    "    with name `public_name` that calls class method `method_name`.\n",
    "    \"\"\"\n",
    "    args_sig = \", \".join(arg_names)\n",
    "    call_sig = \", \".join(arg_names)\n",
    "    wrapper = f\"\\n\\ndef {public_name}({args_sig}):\\n    return {class_name}().{method_name}({call_sig})\\n\"\n",
    "    return class_src.rstrip() + wrapper\n",
    "\n",
    "def normalize_tests(tests, spec):\n",
    "    \"\"\"\n",
    "    Normalize test cases' inputs to match the public function signature.\n",
    "    spec: list describing the expected argument shapes, e.g.\n",
    "      - ['list']            => one positional arg that is a list\n",
    "      - ['list','list']     => two positional args that are lists\n",
    "      - ['str'] or ['str','str']\n",
    "      - ['int'] etc.\n",
    "    Returns a new list of tests with corrected 'inputs'.\n",
    "    \"\"\"\n",
    "    norm = []\n",
    "    for t in tests:\n",
    "        inp = t['inputs']\n",
    "        # Always work with a list of positional args\n",
    "        args = inp if isinstance(inp, (list, tuple)) else [inp]\n",
    "\n",
    "        # If spec is one-list, but args is a flat list of ints, wrap it\n",
    "        if spec == ['list']:\n",
    "            if len(args) == 1 and not isinstance(args[0], list):\n",
    "                # single non-list → keep as is (user passed scalar)\n",
    "                pass\n",
    "            elif len(args) == 0:\n",
    "                args = [[]]\n",
    "            elif len(args) == 1 and isinstance(args[0], list):\n",
    "                pass  # already correct\n",
    "            else:\n",
    "                # user passed a flattened sequence instead of one list: wrap\n",
    "                args = [list(args)]\n",
    "\n",
    "        elif spec == ['list', 'list']:\n",
    "            # Ensure exactly two lists\n",
    "            if len(args) == 2 and all(isinstance(a, list) for a in args):\n",
    "                pass\n",
    "            elif len(args) == 1 and isinstance(args[0], list) and len(args[0]) == 2 and all(isinstance(a, list) for a in args[0]):\n",
    "                args = args[0]  # user nested [[list1, list2]]\n",
    "            else:\n",
    "                # Best-effort: if not two lists, keep as-is (will fail fast)\n",
    "                args = list(args)\n",
    "\n",
    "        elif spec == ['str']:\n",
    "            if len(args) != 1 or not isinstance(args[0], str):\n",
    "                # coerce to single string if possible\n",
    "                args = [\"\".join(map(str, args))]\n",
    "        elif spec == ['str','str']:\n",
    "            if len(args) == 2 and all(isinstance(a, str) for a in args):\n",
    "                pass\n",
    "            elif len(args) == 1 and isinstance(args[0], (list, tuple)) and len(args[0]) == 2:\n",
    "                args = list(args[0])\n",
    "            else:\n",
    "                # crude fallback\n",
    "                s = \"\".join(map(str, args))\n",
    "                args = [s, \"\"]\n",
    "        # leave other specs as-is\n",
    "\n",
    "        norm.append({**t, 'inputs': args})\n",
    "    return norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "248a4a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your class-based candidate (example skeleton)\n",
    "container_class_src = \"\"\"\\\n",
    "from typing import List\n",
    "\n",
    "class Solution:\n",
    "    def maxArea(self, height: List[int]) -> int:\n",
    "        i, j = 0, len(height) - 1\n",
    "        best = 0\n",
    "        while i < j:\n",
    "            best = max(best, min(height[i], height[j]) * (j - i))\n",
    "            if height[i] < height[j]:\n",
    "                i += 1\n",
    "            else:\n",
    "                j -= 1\n",
    "        return best\n",
    "\"\"\"\n",
    "\n",
    "# Build wrapper: public function name must match what you send as function_name\n",
    "container_code = build_wrapper_code(\n",
    "    class_src=container_class_src,\n",
    "    class_name=\"Solution\",\n",
    "    method_name=\"maxArea\",\n",
    "    public_name=\"maxArea\",\n",
    "    arg_names=[\"height\"],\n",
    ")\n",
    "\n",
    "# Fix tests: spec is one list positional arg\n",
    "# If you already have tests, do: container_tests_fixed = normalize_tests(container_tests, ['list'])\n",
    "# Example tests:\n",
    "container_tests = [\n",
    "    {\"test_id\":\"t1\",\"inputs\":[[1,8,6,2,5,4,8,3,7]],\"expected_output\":49},\n",
    "    {\"test_id\":\"t2\",\"inputs\":[[1,1]],\"expected_output\":1},\n",
    "    {\"test_id\":\"t3\",\"inputs\":[[2,3,10,5,7,8,9]],\"expected_output\":36},\n",
    "]\n",
    "container_tests_fixed = normalize_tests(container_tests, ['list'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ecc898f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "median_class_src = \"\"\"\\\n",
    "from typing import List\n",
    "\n",
    "class Solution:\n",
    "    def findMedianSortedArrays(self, nums1: List[int], nums2: List[int]) -> float:\n",
    "        A, B = nums1, nums2\n",
    "        m, n = len(A), len(B)\n",
    "        if m > n: A, B, m, n = B, A, n, m\n",
    "        total, half = m + n, (m + n)//2\n",
    "        lo, hi = 0, m\n",
    "        while lo <= hi:\n",
    "            i = (lo + hi)//2\n",
    "            j = half - i\n",
    "            Aleft = A[i-1] if i>0 else float('-inf')\n",
    "            Aright = A[i] if i<m else float('inf')\n",
    "            Bleft = B[j-1] if j>0 else float('-inf')\n",
    "            Bright = B[j] if j<n else float('inf')\n",
    "            if Aleft <= Bright and Bleft <= Aright:\n",
    "                if total % 2:\n",
    "                    return float(min(Aright, Bright))\n",
    "                return (max(Aleft, Bleft) + min(Aright, Bright)) / 2.0\n",
    "            elif Aleft > Bright:\n",
    "                hi = i - 1\n",
    "            else:\n",
    "                lo = i + 1\n",
    "        raise ValueError(\"invalid\")\n",
    "\"\"\"\n",
    "\n",
    "median_code = build_wrapper_code(\n",
    "    class_src=median_class_src,\n",
    "    class_name=\"Solution\",\n",
    "    method_name=\"findMedianSortedArrays\",\n",
    "    public_name=\"findMedianSortedArrays\",\n",
    "    arg_names=[\"nums1\",\"nums2\"],\n",
    ")\n",
    "\n",
    "# Example tests; normalize to two list args\n",
    "median_tests = [\n",
    "    {\"test_id\":\"m1\",\"inputs\":[[1,3],[2]],\"expected_output\":2.0},\n",
    "    {\"test_id\":\"m2\",\"inputs\":[[1,2],[3,4]],\"expected_output\":2.5},\n",
    "]\n",
    "median_tests_fixed = normalize_tests(median_tests, ['list','list'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eef3577e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'candidate_id': None,\n",
      " 'energy_metrics': {'energy_efficiency_score': 0.38000000000000006,\n",
      "                    'energy_per_test_case_joules': 0.38000000000000006,\n",
      "                    'median_execution_time_seconds': 1.2235002941451967e-05,\n",
      "                    'median_package_energy_joules': 1.06,\n",
      "                    'median_ram_energy_joules': 0.08,\n",
      "                    'median_total_energy_joules': 1.1400000000000001,\n",
      "                    'power_consumption_watts': 93175.29431380036},\n",
      " 'measurement_environment': {'cpu_model': None,\n",
      "                             'measurement_core': None,\n",
      "                             'meter_type': 'unknown',\n",
      "                             'thermal_controlled': True,\n",
      "                             'timestamp': 1759186402.717732},\n",
      " 'measurement_timestamp': 1759186402.717732,\n",
      " 'problem_name': None,\n",
      " 'processing_time_seconds': 0.2545042037963867,\n",
      " 'request_id': '70302eb8-2088-4411-82d2-bed12a0a75b9',\n",
      " 'status': 'completed',\n",
      " 'validation': {'error_summary': None,\n",
      "                'is_correct': True,\n",
      "                'pass_rate': 100.0,\n",
      "                'passed_tests': 3,\n",
      "                'total_tests': 3}}\n",
      "{'candidate_id': None,\n",
      " 'energy_metrics': {'energy_efficiency_score': 0.77,\n",
      "                    'energy_per_test_case_joules': 0.77,\n",
      "                    'median_execution_time_seconds': 1.7102996935136616e-05,\n",
      "                    'median_package_energy_joules': 1.36,\n",
      "                    'median_ram_energy_joules': 0.18,\n",
      "                    'median_total_energy_joules': 1.54,\n",
      "                    'power_consumption_watts': 90042.69870599137},\n",
      " 'measurement_environment': {'cpu_model': None,\n",
      "                             'measurement_core': None,\n",
      "                             'meter_type': 'unknown',\n",
      "                             'thermal_controlled': True,\n",
      "                             'timestamp': 1759186403.263865},\n",
      " 'measurement_timestamp': 1759186403.263865,\n",
      " 'problem_name': None,\n",
      " 'processing_time_seconds': 0.27260756492614746,\n",
      " 'request_id': 'ee26f8e6-124a-4e68-8ec7-051e166103df',\n",
      " 'status': 'completed',\n",
      " 'validation': {'error_summary': None,\n",
      "                'is_correct': True,\n",
      "                'pass_rate': 100.0,\n",
      "                'passed_tests': 2,\n",
      "                'total_tests': 2}}\n"
     ]
    }
   ],
   "source": [
    "import requests, time\n",
    "from pprint import pprint\n",
    "\n",
    "BASE_URL = \"http://127.0.0.1:8000\"\n",
    "MEASURE_URL = f\"{BASE_URL}/api/v1/measure\"\n",
    "TASK_URL = f\"{BASE_URL}/api/v1/tasks\"\n",
    "\n",
    "def queue_job(code, function_name, tests, timeout=10, trials=3, warmup=1, mem_mb=1024):\n",
    "    payload = {\n",
    "        \"candidate_code\": code,\n",
    "        \"function_name\": function_name,\n",
    "        \"test_cases\": tests,\n",
    "        \"timeout_seconds\": timeout,\n",
    "        \"memory_limit_mb\": mem_mb,\n",
    "        \"energy_measurement_trials\": trials,\n",
    "        \"warmup_trials\": warmup,\n",
    "    }\n",
    "    r = requests.post(MEASURE_URL, json=payload, timeout=60)\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "def poll_task(task_id):\n",
    "    while True:\n",
    "        r = requests.get(f\"{TASK_URL}/{task_id}\", timeout=60)\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "        if data.get(\"status\") in {\"completed\", \"failed\"}:\n",
    "            return data\n",
    "        time.sleep(0.5)\n",
    "\n",
    "# Container With Most Water\n",
    "q1 = queue_job(container_code, \"maxArea\", container_tests_fixed, timeout=10, trials=3, warmup=1)\n",
    "r1 = poll_task(q1[\"task_id\"]); pprint(r1)\n",
    "\n",
    "# Median of Two Sorted Arrays\n",
    "q2 = queue_job(median_code, \"findMedianSortedArrays\", median_tests_fixed, timeout=10, trials=3, warmup=1)\n",
    "r2 = poll_task(q2[\"task_id\"]); pprint(r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6277e37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "763ccbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import requests\n",
    "from urllib.parse import urljoin\n",
    "from pprint import pprint\n",
    "\n",
    "# Base URL for the running JouleTrace API\n",
    "JOULETRACE_BASE_URL = os.getenv(\"JOULETRACE_BASE_URL\", \"http://127.0.0.1:8000\")\n",
    "MEASURE_URL = f\"{JOULETRACE_BASE_URL}/api/v1/measure\"\n",
    "\n",
    "def poll_from_poll_url(poll_url: str, timeout_s: int = 600, interval_s: float = 0.5):\n",
    "    url = poll_url if poll_url.startswith(\"http\") else urljoin(JOULETRACE_BASE_URL, poll_url)\n",
    "    s = requests.Session()\n",
    "    start = time.time()\n",
    "    last_status = None\n",
    "    while True:\n",
    "        r = s.get(url, timeout=30)\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "        status = data.get(\"status\")\n",
    "        if status != last_status or status == \"running\":\n",
    "            if status == \"running\":\n",
    "                print(f\"status=running stage={data.get('stage')} progress={data.get('progress')}\")\n",
    "            else:\n",
    "                print(f\"status={status}\")\n",
    "            last_status = status\n",
    "        if status in {\"completed\", \"failed\"}:\n",
    "            return data\n",
    "        if time.time() - start > timeout_s:\n",
    "            raise TimeoutError(f\"Polling timed out after {timeout_s}s: {url}\")\n",
    "        time.sleep(interval_s)\n",
    "\n",
    "def queue_job(candidate_code: str, function_name: str, test_cases: list,\n",
    "              timeout_seconds=10, memory_limit_mb=1024,\n",
    "              trials=3, warmup=1):\n",
    "    payload = {\n",
    "        \"candidate_code\": candidate_code,\n",
    "        \"function_name\": function_name,\n",
    "        \"test_cases\": test_cases,\n",
    "        \"timeout_seconds\": timeout_seconds,\n",
    "        \"memory_limit_mb\": memory_limit_mb,\n",
    "        \"energy_measurement_trials\": trials,\n",
    "        \"warmup_trials\": warmup,\n",
    "    }\n",
    "    r = requests.post(MEASURE_URL, json=payload, timeout=60)\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "038214a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'expected_output': 17711, 'inputs': [22], 'test_id': 't-22'},\n",
      " {'expected_output': 46368, 'inputs': [24], 'test_id': 't-24'}]\n"
     ]
    }
   ],
   "source": [
    "# Pure Python local reference to produce expected outputs\n",
    "def fib_ref(n: int) -> int:\n",
    "    a, b = 0, 1\n",
    "    for _ in range(n):\n",
    "        a, b = b, a + b\n",
    "    return a\n",
    "\n",
    "# Candidate: fast iterative (O(n))\n",
    "candidate_fast = \"\"\"\\\n",
    "def solve(n):\n",
    "    a, b = 0, 1\n",
    "    for _ in range(n):\n",
    "        a, b = b, a + b\n",
    "    return a\n",
    "\"\"\"\n",
    "\n",
    "# Candidate: slow naive recursion (exponential) — keep n modest so it finishes\n",
    "candidate_slow = \"\"\"\\\n",
    "def solve(n):\n",
    "    if n < 2:\n",
    "        return n\n",
    "    return solve(n-1) + solve(n-2)\n",
    "\"\"\"\n",
    "\n",
    "# Build the same tests for both candidates\n",
    "# Choose ns so slow version finishes within timeout; adjust if needed\n",
    "ns = [22, 24]  # both should complete for naive recursion with a reasonable timeout\n",
    "tests = [\n",
    "    {\"test_id\": f\"t-{n}\", \"inputs\": [n], \"expected_output\": fib_ref(n)}\n",
    "    for n in ns\n",
    "]\n",
    "pprint(tests)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4eefe5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def energy_reward(result_json: dict,\n",
    "                  ref_energy_j: float = 1.0,\n",
    "                  ref_time_s: float = 0.01,\n",
    "                  beta_energy: float = 1.5,\n",
    "                  k_energy: float = 1.2,\n",
    "                  k_time: float = 1.0,\n",
    "                  hard_cap_multiple: float = 50.0) -> float:\n",
    "    \"\"\"Reward in [0, 1] with harmonic emphasis on the worse dimension.\"\"\"\n",
    "    if result_json.get(\"status\") != \"completed\":\n",
    "        return 0.0\n",
    "\n",
    "    validation = result_json.get(\"validation\") or {}\n",
    "    if not validation.get(\"is_correct\", False):\n",
    "        return 0.0\n",
    "\n",
    "    em = result_json.get(\"energy_metrics\") or {}\n",
    "    E = em.get(\"median_total_energy_joules\")\n",
    "    T = em.get(\"median_execution_time_seconds\")\n",
    "    if E is None or T is None or E < 0 or T < 0:\n",
    "        return 0.0\n",
    "\n",
    "    if E > hard_cap_multiple * ref_energy_j or T > hard_cap_multiple * ref_time_s:\n",
    "        return 0.0\n",
    "\n",
    "    safe_ref_energy = max(ref_energy_j, 1e-12)\n",
    "    safe_ref_time = max(ref_time_s, 1e-12)\n",
    "\n",
    "    rE = E / safe_ref_energy\n",
    "    rT = T / safe_ref_time\n",
    "\n",
    "    Se = 1.0 / (1.0 + (rE ** k_energy))\n",
    "    St = 1.0 / (1.0 + (rT ** k_time))\n",
    "\n",
    "    beta2 = beta_energy * beta_energy\n",
    "    denom = beta2 * Se + St\n",
    "    if denom <= 0.0:\n",
    "        return 0.0\n",
    "\n",
    "    reward = (1.0 + beta2) * (Se * St) / denom\n",
    "    if not (reward == reward):\n",
    "        return 0.0\n",
    "\n",
    "    return float(max(0.0, min(1.0, reward)))\n",
    "\n",
    "\n",
    "def summarize_energy(result_json: dict) -> dict:\n",
    "    em = result_json.get(\"energy_metrics\") or {}\n",
    "    return {\n",
    "        \"package_J\": em.get(\"median_package_energy_joules\"),\n",
    "        \"ram_J\": em.get(\"median_ram_energy_joules\"),\n",
    "        \"total_J\": em.get(\"median_total_energy_joules\"),\n",
    "        \"time_s\": em.get(\"median_execution_time_seconds\"),\n",
    "        \"power_W\": em.get(\"power_consumption_watts\"),\n",
    "    }\n",
    "\n",
    "\n",
    "def _format_with_units(value, thresholds_units, scale):\n",
    "    if value is None:\n",
    "        return \"n/a\"\n",
    "    abs_value = abs(value)\n",
    "    for threshold, unit in thresholds_units:\n",
    "        if abs_value >= threshold:\n",
    "            return f\"{value / scale[unit]:.3f} {unit}\"\n",
    "    unit = thresholds_units[-1][1]\n",
    "    return f\"{value / scale[unit]:.3f} {unit}\"\n",
    "\n",
    "\n",
    "def format_energy(value):\n",
    "    thresholds = ((1.0, \"J\"), (1e-3, \"mJ\"), (1e-6, \"uJ\"), (1e-9, \"nJ\"), (0.0, \"pJ\"))\n",
    "    scale = {\"J\": 1.0, \"mJ\": 1e-3, \"uJ\": 1e-6, \"nJ\": 1e-9, \"pJ\": 1e-12}\n",
    "    return _format_with_units(value, thresholds, scale)\n",
    "\n",
    "\n",
    "def format_time(value):\n",
    "    thresholds = ((1.0, \"s\"), (1e-3, \"ms\"), (1e-6, \"us\"), (1e-9, \"ns\"), (0.0, \"ps\"))\n",
    "    scale = {\"s\": 1.0, \"ms\": 1e-3, \"us\": 1e-6, \"ns\": 1e-9, \"ps\": 1e-12}\n",
    "    return _format_with_units(value, thresholds, scale)\n",
    "\n",
    "\n",
    "def format_power(value):\n",
    "    thresholds = ((1.0, \"W\"), (1e-3, \"mW\"), (1e-6, \"uW\"), (1e-9, \"nW\"), (0.0, \"pW\"))\n",
    "    scale = {\"W\": 1.0, \"mW\": 1e-3, \"uW\": 1e-6, \"nW\": 1e-9, \"pW\": 1e-12}\n",
    "    return _format_with_units(value, thresholds, scale)\n",
    "\n",
    "\n",
    "def pad(text: str, width: int = 20) -> str:\n",
    "    return f\"{text:<{width}}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e00aa9e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queuing slow...\n",
      "Queuing fast...\n",
      "\n",
      "Polling slow...\n",
      "status=running stage=validation progress=0.1\n",
      "status=completed\n",
      "\n",
      "Polling fast...\n",
      "status=completed\n",
      "\n",
      "Slow result:\n",
      "{'candidate_id': None,\n",
      " 'energy_metrics': {'energy_efficiency_score': 1.1099999999999999,\n",
      "                    'energy_per_test_case_joules': 1.1099999999999999,\n",
      "                    'median_execution_time_seconds': 0.009937327995430678,\n",
      "                    'median_package_energy_joules': 1.97,\n",
      "                    'median_ram_energy_joules': 0.25,\n",
      "                    'median_total_energy_joules': 2.2199999999999998,\n",
      "                    'power_consumption_watts': 223.40009316596843},\n",
      " 'measurement_environment': {'cpu_model': None,\n",
      "                             'measurement_core': None,\n",
      "                             'meter_type': 'unknown',\n",
      "                             'thermal_controlled': True,\n",
      "                             'timestamp': 1759186417.706287},\n",
      " 'measurement_timestamp': 1759186417.706287,\n",
      " 'problem_name': None,\n",
      " 'processing_time_seconds': 0.3116893768310547,\n",
      " 'request_id': 'a8650a58-3cd6-4083-9fe9-c5804f0933b6',\n",
      " 'status': 'completed',\n",
      " 'validation': {'error_summary': None,\n",
      "                'is_correct': True,\n",
      "                'pass_rate': 100.0,\n",
      "                'passed_tests': 2,\n",
      "                'total_tests': 2}}\n",
      "\n",
      "Fast result:\n",
      "{'candidate_id': None,\n",
      " 'energy_metrics': {'energy_efficiency_score': 0.735,\n",
      "                    'energy_per_test_case_joules': 0.735,\n",
      "                    'median_execution_time_seconds': 6.090995157137513e-06,\n",
      "                    'median_package_energy_joules': 1.3,\n",
      "                    'median_ram_energy_joules': 0.17,\n",
      "                    'median_total_energy_joules': 1.47,\n",
      "                    'power_consumption_watts': 241339.87338299447},\n",
      " 'measurement_environment': {'cpu_model': None,\n",
      "                             'measurement_core': None,\n",
      "                             'meter_type': 'unknown',\n",
      "                             'thermal_controlled': True,\n",
      "                             'timestamp': 1759186417.658981},\n",
      " 'measurement_timestamp': 1759186417.658981,\n",
      " 'problem_name': None,\n",
      " 'processing_time_seconds': 0.25359272956848145,\n",
      " 'request_id': '6a41e9f2-c287-4a64-bff4-ce54ea7e413d',\n",
      " 'status': 'completed',\n",
      " 'validation': {'error_summary': None,\n",
      "                'is_correct': True,\n",
      "                'pass_rate': 100.0,\n",
      "                'passed_tests': 2,\n",
      "                'total_tests': 2}}\n"
     ]
    }
   ],
   "source": [
    "print(\"Queuing slow...\")\n",
    "q_slow = queue_job(candidate_slow, \"solve\", tests, timeout_seconds=15, trials=3, warmup=1)\n",
    "print(\"Queuing fast...\")\n",
    "q_fast = queue_job(candidate_fast, \"solve\", tests, timeout_seconds=15, trials=3, warmup=1)\n",
    "\n",
    "print(\"\\nPolling slow...\")\n",
    "r_slow = poll_from_poll_url(q_slow[\"poll_url\"])\n",
    "print(\"\\nPolling fast...\")\n",
    "r_fast = poll_from_poll_url(q_fast[\"poll_url\"])\n",
    "\n",
    "print(\"\\nSlow result:\")\n",
    "pprint(r_slow)\n",
    "print(\"\\nFast result:\")\n",
    "pprint(r_fast)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7954b3dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slow energy: {'package_J': 1.96, 'ram_J': 0.25, 'total_J': 2.21, 'time_s': 0.00993454000854399, 'power_W': 222.4561980825822} reward: 0.40247187835817677\n",
      "Fast energy: {'package_J': 1.25, 'ram_J': 0.17, 'total_J': 1.43, 'time_s': 5.997993866913021e-06, 'power_W': 238413.048050677} reward: 0.6788664759891463\n",
      "==================================================================\n",
      "                    ENERGY EFFICIENCY SUMMARY                     \n",
      "==================================================================\n",
      "Candidate   Status      Reward      Energy         Time           \n",
      "------------------------------------------------------------------\n",
      "slow        completed   0.4025      2.210 J        9.935 ms       \n",
      "fast        completed   0.6789      1.430 J        5.998 us       \n",
      "------------------------------------------------------------------\n",
      "Winner: fast\n"
     ]
    }
   ],
   "source": [
    "slow_energy = summarize_energy(r_slow)\n",
    "fast_energy = summarize_energy(r_fast)\n",
    "slow_reward = energy_reward(r_slow)\n",
    "fast_reward = energy_reward(r_fast)\n",
    "\n",
    "print(\"Slow energy:\", slow_energy, \"reward:\", slow_reward)\n",
    "print(\"Fast energy:\", fast_energy, \"reward:\", fast_reward)\n",
    "\n",
    "rows = {\n",
    "    \"slow\": {\n",
    "        \"status\": r_slow.get(\"status\"),\n",
    "        \"energy\": slow_energy,\n",
    "        \"reward\": slow_reward,\n",
    "    },\n",
    "    \"fast\": {\n",
    "        \"status\": r_fast.get(\"status\"),\n",
    "        \"energy\": fast_energy,\n",
    "        \"reward\": fast_reward,\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\"\" + \"=\" * 66)\n",
    "print(\"ENERGY EFFICIENCY SUMMARY\".center(66))\n",
    "print(\"=\" * 66)\n",
    "print(f\"{'Candidate':<12}{'Status':<12}{'Reward':<12}{'Energy':<15}{'Time':<15}\")\n",
    "print(\"-\" * 66)\n",
    "for name, info in rows.items():\n",
    "    energy = info[\"energy\"]\n",
    "    print(f\"{name:<12}\"\n",
    "          f\"{str(info['status']):<12}\"\n",
    "          f\"{info['reward']:<12.4f}\"\n",
    "          f\"{format_energy(energy.get('total_J')):<15}\"\n",
    "          f\"{format_time(energy.get('time_s')):<15}\")\n",
    "print(\"-\" * 66)\n",
    "print(f\"Winner: {'fast' if fast_reward > slow_reward else ('slow' if slow_reward > fast_reward else 'tie')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "70eb60b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "status=running stage=validation progress=0.1\n",
      "status=completed\n",
      "status=completed\n",
      "slow=0.406 fast=0.666\n",
      "status=running stage=validation progress=0.1\n",
      "status=completed\n",
      "status=completed\n",
      "slow=0.377 fast=0.639\n",
      "status=running stage=validation progress=0.1\n",
      "status=completed\n",
      "status=completed\n",
      "slow=0.409 fast=0.672\n",
      "Averaged rewards over 3 runs:\n",
      "slow avg: 0.39727016963377926\n",
      "fast avg: 0.6588473184449485\n"
     ]
    }
   ],
   "source": [
    "def run_once():\n",
    "    q1 = queue_job(candidate_slow, \"solve\", tests, timeout_seconds=15, trials=3, warmup=1)\n",
    "    q2 = queue_job(candidate_fast, \"solve\", tests, timeout_seconds=15, trials=3, warmup=1)\n",
    "    r1 = poll_from_poll_url(q1[\"poll_url\"])\n",
    "    r2 = poll_from_poll_url(q2[\"poll_url\"])\n",
    "    return energy_reward(r1), energy_reward(r2)\n",
    "\n",
    "repeats = 3\n",
    "slow_rs, fast_rs = [], []\n",
    "for _ in range(repeats):\n",
    "    sr, fr = run_once()\n",
    "    slow_rs.append(sr)\n",
    "    fast_rs.append(fr)\n",
    "    print(f\"slow={sr:.3f} fast={fr:.3f}\")\n",
    "\n",
    "print(\"Averaged rewards over\", repeats, \"runs:\")\n",
    "print(\"slow avg:\", sum(slow_rs)/len(slow_rs))\n",
    "print(\"fast avg:\", sum(fast_rs)/len(fast_rs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df7080a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "greenllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
